{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\envs\\dig_health\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "import draft.data_cleaning as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_1 = pd.read_json(r'datasets/december/liza.json', orient='records', lines=True) # liza\n",
    "person_2 = pd.read_json(r'datasets/december/sleep_data_Adham.json', lines=True) # adham\n",
    "person_3 = pd.read_json(r'datasets/december/sleep_data_Miriam.json', lines=True) # miriam\n",
    "person_4 = pd.read_json(r'datasets/december/sleep_data_Syahid.json', lines=True) # syahid\n",
    "\n",
    "# labels dataframe from excel\n",
    "labels_df = pd.read_excel(r'datasets\\sleep_data.xlsx', sheet_name=None) # dict of all label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = [person_1, person_2, person_3, person_4]\n",
    "\n",
    "for num, df in enumerate(people_df):\n",
    "    df.insert(0, \"temp_id\", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchlaufen der Liste und Anwenden der Funktionen\n",
    "for num, df in enumerate(people_df):\n",
    "    # Entfernen unverfolgter Nächte\n",
    "    people_df[num] = dc.delete_untracked_nights(df)\n",
    "\n",
    "    # Zurücksetzen des Index\n",
    "    people_df[num].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Optional: Zuweisung der bearbeiteten DataFrames zu ihren ursprünglichen Variablennamen\n",
    "person_1, person_2, person_3, person_4 = people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_list = []\n",
    "\n",
    "for num, df in enumerate(people_df):\n",
    "    # Extrahieren der Zeitreihendaten (Annahme: gibt einen DataFrame zurück)\n",
    "    temp_df = dc.main_interpolation(df)\n",
    "\n",
    "    # Weisen Sie die eindeutige ID dem DataFrame zu\n",
    "    temp_df['temp_id'] = num\n",
    "\n",
    "    # Fügen Sie den aktualisierten DataFrame der neuen Liste hinzu\n",
    "    time_series_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_list[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1:\n",
      "  Nacht 0: 626 Zeilen\n",
      "  Nacht 1: 542 Zeilen\n",
      "  Nacht 2: 428 Zeilen\n",
      "  Nacht 3: 359 Zeilen\n",
      "  Nacht 4: 520 Zeilen\n",
      "  Nacht 5: 413 Zeilen\n",
      "  Nacht 6: 548 Zeilen\n",
      "  Nacht 7: 667 Zeilen\n",
      "  Nacht 8: 428 Zeilen\n",
      "  Nacht 9: 522 Zeilen\n",
      "  Nacht 10: 396 Zeilen\n",
      "  Nacht 11: 541 Zeilen\n",
      "  Nacht 12: 437 Zeilen\n",
      "  Nacht 13: 415 Zeilen\n",
      "  Nacht 14: 552 Zeilen\n",
      "Person 2:\n",
      "  Nacht 0: 476 Zeilen\n",
      "  Nacht 1: 355 Zeilen\n",
      "  Nacht 2: 344 Zeilen\n",
      "  Nacht 3: 454 Zeilen\n",
      "  Nacht 4: 313 Zeilen\n",
      "  Nacht 5: 529 Zeilen\n",
      "  Nacht 6: 254 Zeilen\n",
      "  Nacht 7: 320 Zeilen\n",
      "  Nacht 8: 600 Zeilen\n",
      "  Nacht 9: 301 Zeilen\n",
      "  Nacht 10: 584 Zeilen\n",
      "  Nacht 11: 384 Zeilen\n",
      "  Nacht 12: 478 Zeilen\n",
      "  Nacht 13: 478 Zeilen\n",
      "  Nacht 14: 434 Zeilen\n",
      "  Nacht 15: 324 Zeilen\n",
      "  Nacht 16: 562 Zeilen\n",
      "  Nacht 17: 505 Zeilen\n",
      "  Nacht 18: 394 Zeilen\n",
      "Person 3:\n",
      "  Nacht 0: 579 Zeilen\n",
      "  Nacht 1: 313 Zeilen\n",
      "  Nacht 2: 495 Zeilen\n",
      "  Nacht 3: 455 Zeilen\n",
      "  Nacht 4: 547 Zeilen\n",
      "  Nacht 5: 481 Zeilen\n",
      "  Nacht 6: 458 Zeilen\n",
      "  Nacht 7: 524 Zeilen\n",
      "  Nacht 8: 314 Zeilen\n",
      "  Nacht 9: 328 Zeilen\n",
      "  Nacht 10: 487 Zeilen\n",
      "  Nacht 11: 347 Zeilen\n",
      "  Nacht 12: 407 Zeilen\n",
      "  Nacht 13: 435 Zeilen\n",
      "  Nacht 14: 310 Zeilen\n",
      "Person 4:\n",
      "  Nacht 0: 646 Zeilen\n",
      "  Nacht 1: 463 Zeilen\n",
      "  Nacht 2: 534 Zeilen\n",
      "  Nacht 3: 701 Zeilen\n",
      "  Nacht 4: 438 Zeilen\n",
      "  Nacht 5: 371 Zeilen\n",
      "  Nacht 6: 603 Zeilen\n",
      "  Nacht 7: 367 Zeilen\n",
      "  Nacht 8: 542 Zeilen\n",
      "  Nacht 9: 212 Zeilen\n",
      "  Nacht 10: 411 Zeilen\n",
      "  Nacht 11: 384 Zeilen\n",
      "  Nacht 12: 315 Zeilen\n",
      "  Nacht 13: 460 Zeilen\n",
      "  Nacht 14: 516 Zeilen\n",
      "  Nacht 15: 405 Zeilen\n",
      "  Nacht 16: 453 Zeilen\n",
      "  Nacht 17: 994 Zeilen\n",
      "  Nacht 18: 470 Zeilen\n",
      "  Nacht 19: 695 Zeilen\n",
      "  Nacht 20: 322 Zeilen\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# find total nights\n",
    "total_nights = 0\n",
    "# Durchlaufen jeder Person in der Liste\n",
    "for person_index, person_data in enumerate(time_series_list):\n",
    "    print(f\"Person {person_index + 1}:\")\n",
    "\n",
    "    # Überprüfen, ob person_data ein Dictionary ist\n",
    "    if isinstance(person_data, dict):\n",
    "        # Durchlaufen jedes Nacht-DataFrames der Person\n",
    "        for night, df in person_data.items():\n",
    "            # Überprüfen, ob der Wert ein DataFrame ist\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                #if len(df) > max_length:\n",
    "                    #max_length = len(df)\n",
    "                    print(f\"  Nacht {night}: {len(df)} Zeilen\")\n",
    "                    total_nights +=1\n",
    "\n",
    "print(total_nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maximum_length(list_):\n",
    "    max_length = 0  # Initialize max_length to store the maximum number of rows\n",
    "\n",
    "    # Iterate through each person in the list\n",
    "    for person_index, person_data in enumerate(time_series_list):\n",
    "        # Iterate through each night DataFrame of the person\n",
    "        for night, df in person_data.items():\n",
    "            # Check if the value is a DataFrame\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                # Update max_length if this night has more rows\n",
    "                if len(df) > max_length:\n",
    "                    max_length = len(df)\n",
    "\n",
    "    # After completing the iteration, max_length will hold the number of rows of the longest night\n",
    "    return max_length\n",
    "\n",
    "max_length = calculate_maximum_length(time_series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\envs\\dig_health\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 994, 7)]          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               43200     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 994, 100)          0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 994, 100)          80400     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 994, 7)            3024      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126624 (494.62 KB)\n",
      "Trainable params: 126624 (494.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "inputs = Input(shape=(max_length, n_features))\n",
    "encoder_lstm = LSTM(100, activation='relu', return_sequences=False)(inputs)\n",
    "encoder_output = RepeatVector(max_length)(encoder_lstm)\n",
    "\n",
    "# Decoder\n",
    "decoder_lstm = LSTM(100, activation='relu', return_sequences=True)(encoder_output)\n",
    "decoder_output = LSTM(n_features, return_sequences=True)(decoder_lstm)\n",
    "\n",
    "# Seq2Seq Autoencoder Model\n",
    "seq2seq_autoencoder = Model(inputs, decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "seq2seq_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model summary\n",
    "seq2seq_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.0965 - val_loss: 0.1095\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0948 - val_loss: 0.1067\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0925 - val_loss: 0.1031\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0896 - val_loss: 0.0984\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0858 - val_loss: 0.0921\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0809 - val_loss: 0.0846\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0752 - val_loss: 0.0775\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0701 - val_loss: 0.0861\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0672 - val_loss: 0.0785\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0638 - val_loss: 0.0664\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0619 - val_loss: 0.0661\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0610 - val_loss: 0.0658\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0607 - val_loss: 0.0661\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0610 - val_loss: 0.0664\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0611 - val_loss: 0.0657\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0605 - val_loss: 0.0650\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0602 - val_loss: 0.0647\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0601 - val_loss: 0.0645\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0602 - val_loss: 0.0643\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0601 - val_loss: 0.0642\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0600 - val_loss: 0.0641\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0600 - val_loss: 0.0642\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0600 - val_loss: 0.0642\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0600 - val_loss: 0.0642\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0642\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0643\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0599 - val_loss: 0.0642\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0599 - val_loss: 0.0641\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0642\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0641\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0597 - val_loss: 0.0642\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0598 - val_loss: 0.0642\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0597 - val_loss: 0.0640\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0597 - val_loss: 0.0640\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.0597 - val_loss: 0.0641\n"
     ]
    }
   ],
   "source": [
    "# Define a generator function to yield night DataFrames\n",
    "def night_generator(time_series_list):\n",
    "    for person_data in time_series_list:\n",
    "        for night, df in person_data.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                yield df\n",
    "\n",
    "# Define a function to process data\n",
    "def process_data(interpolated_df, max_length):\n",
    "    padded_data = pad_sequences([interpolated_df.values], dtype='float32', padding='post', maxlen=max_length)\n",
    "    return padded_data[0]\n",
    "\n",
    "# Iterate through each person and night to preprocess and pad the data\n",
    "\n",
    "#Define the maximum length\n",
    "max_length = calculate_maximum_length(time_series_list)\n",
    "\n",
    "night_iterator = night_generator(time_series_list)\n",
    "processed_data = []\n",
    "\n",
    "for night_df in night_iterator:\n",
    "    padded_night_data = process_data(night_df, max_length) # do the padding\n",
    "    processed_data.append(padded_night_data)\n",
    "\n",
    "# Convert processed data to a NumPy array and normalize if needed\n",
    "training_data = np.array(processed_data)\n",
    "\n",
    "# Define the number of features per timestep\n",
    "n_features = 7\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(max_length, n_features))\n",
    "encoder_lstm = LSTM(100, activation='relu', return_sequences=False)(inputs)\n",
    "encoder_output = RepeatVector(max_length)(encoder_lstm)\n",
    "\n",
    "# Decoder\n",
    "decoder_lstm = LSTM(100, activation='relu', return_sequences=True)(encoder_output)\n",
    "decoder_output = LSTM(n_features, return_sequences=True)(decoder_lstm)\n",
    "\n",
    "# Seq2Seq Autoencoder Model\n",
    "seq2seq_autoencoder = Model(inputs, decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "seq2seq_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define the number of epochs and batch size\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = seq2seq_autoencoder.fit(training_data, training_data, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 994, 7)]          0         \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 100)               43200     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43200 (168.75 KB)\n",
      "Trainable params: 43200 (168.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Assuming your encoder LSTM layer is named 'encoder_lstm'\n",
    "\n",
    "# Step 1: Extract the Encoder Model\n",
    "# This model takes the same inputs as your seq2seq_autoencoder but outputs the encoder's state\n",
    "encoder_model = Model(inputs=inputs, outputs=encoder_lstm)\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 369ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.03929291, 0.        , 0.05729467, 0.05546304,\n",
       "        0.        , 0.00956762, 0.        , 0.02829745, 0.02150617,\n",
       "        0.00673084, 0.05754669, 0.        , 0.03195135, 0.01586103,\n",
       "        0.01334444, 0.0274994 , 0.        , 0.        , 0.        ,\n",
       "        0.06037556, 0.00967422, 0.        , 0.0394587 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01515129,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01892987,\n",
       "        0.        , 0.        , 0.05186971, 0.03989829, 0.        ,\n",
       "        0.        , 0.        , 0.00200168, 0.        , 0.        ,\n",
       "        0.04898386, 0.03667377, 0.02549991, 0.        , 0.        ,\n",
       "        0.02723131, 0.01746187, 0.        , 0.05198036, 0.03288264,\n",
       "        0.        , 0.        , 0.01830428, 0.04225369, 0.        ,\n",
       "        0.05542608, 0.0306214 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01962096, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02174385, 0.04960664,\n",
       "        0.04373104, 0.        , 0.        , 0.07051437, 0.        ,\n",
       "        0.0331119 , 0.03973646, 0.03810446, 0.02466881, 0.03625934,\n",
       "        0.        , 0.        , 0.0192661 , 0.05101568, 0.03808439,\n",
       "        0.        , 0.        , 0.04363775, 0.        , 0.00816666,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05389245]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Use the Encoder for Prediction\n",
    "# padding the input\n",
    "padded = process_data(time_series_list[0][0], max_length=max_length)\n",
    "arr = np.array(padded)\n",
    "data_reshaped = np.expand_dims(arr, axis=0)\n",
    "\n",
    "# get the embeddings\n",
    "encoder_model.predict(data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 994, 7)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reshaped.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dig_health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
